{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/clips/'\n",
    "train_df = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(1, norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "\n",
    "def encode_trans(trans, char2ind, maxlen_t=7):\n",
    "    '''\n",
    "    Encodes true transcription\n",
    "    trans: \n",
    "    '''\n",
    "    res = np.array([char2ind[char] for char in trans])\n",
    "    res = np.pad(res, (0, maxlen_t-len(res)), 'constant', constant_values=(0))\n",
    "    mask = [1 if i>0 else 0 for i in res]\n",
    "    return torch.tensor(res), torch.tensor(mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, char2ind, transforms, maxlen, maxlent):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.char2ind = char2ind\n",
    "        self.transforms = transforms\n",
    "        self.maxlen = maxlen\n",
    "        self.maxlent = maxlent\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, fmask = self.transforms[0](fname, self.maxlen)\n",
    "        trans, tmask = self.transforms[1](transcript, self.char2ind, self.maxlent)\n",
    "        sample = {'aud': nan_to_num(feat), 'trans': trans, 'fmask':fmask, 'tmask':tmask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize weights randomly\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)\n",
    "\n",
    "def nan_to_num(t,mynan=0.):\n",
    "    if torch.all(torch.isfinite(t)):\n",
    "        return t\n",
    "    if len(t.size()) == 0:\n",
    "        return torch.tensor(mynan)\n",
    "    return torch.cat([nan_to_num(l).unsqueeze(0) for l in t],0)\n",
    "\n",
    "def save_predictions(target, predicted, model_path):\n",
    "    path = os.path.join(model_path, \"predicted.txt\")\n",
    "    with open(path, 'w') as fo:\n",
    "        for i in range(len(target)):\n",
    "            fo.write(target[i] + \"|\" + predicted[i] + '\\n')\n",
    "            \n",
    "def collapse_fn(preds):\n",
    "    seq = ''\n",
    "    prev = ''\n",
    "    for char in preds:\n",
    "        if not prev:\n",
    "            prev = char\n",
    "            seq+=char\n",
    "        if char==prev:\n",
    "            continue\n",
    "        else:\n",
    "            prev=char\n",
    "            seq+=char\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=2,\n",
    "                             dropout=0.3, \n",
    "                             bidirectional=True)\n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            out = self.drop(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        lengths = torch.sum(mask, dim=1).detach().cpu()\n",
    "        outputs = pack_padded_sequence(outputs, lengths, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.blstm(outputs)\n",
    "        output, _ = pad_packed_sequence(output, total_length=mask.shape[1])\n",
    "        return output\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, enc_hid_states, dec_hid, device):\n",
    "        enc_hid_states = torch.transpose(enc_hid_states, 0, 1)\n",
    "        scores = torch.zeros(dec_hid.shape[0], enc_hid_states.shape[0]).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            score_i = torch.bmm(dec_hid.unsqueeze(1), enc_hid.unsqueeze(2))[:,0,0]\n",
    "            scores[:, i] = score_i\n",
    "        \n",
    "        align = F.softmax(scores, dim=1)\n",
    "        c_t = torch.zeros(dec_hid.shape).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            c_t+= align[:, i].unsqueeze(1)*enc_hid\n",
    "        return c_t\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(output_size, 128)\n",
    "        #self.lstm_cell = nn.LSTMCell(128, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=128, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=1,\n",
    "                            dropout=0.3)\n",
    "        self.output = nn.Linear(2* hidden_size, output_size)\n",
    "        self.attention = Attention()\n",
    "        self.drop_lstm = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, target_inputs, encoder_outputs, device=None):\n",
    "        dec_hid = encoder_outputs[-1].unsqueeze(0)\n",
    "\n",
    "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1)\n",
    "        c_i = torch.zeros(dec_hid.shape).to(device)\n",
    "        dec_outputs = []\n",
    "\n",
    "        for inp in torch.transpose(target_inputs, 0, 1):\n",
    "            embedded = self.embed_layer(inp)\n",
    "            dec_out, (dec_hid, _) = self.lstm(embedded.unsqueeze(0), (dec_hid, c_i))\n",
    "            context = self.attention(encoder_outputs, dec_out.squeeze(0), device)\n",
    "            combined_input = torch.cat([dec_hid.squeeze(0), context], 1)\n",
    "            output_i = self.output(combined_input)\n",
    "            output_i = F.log_softmax(output_i, dim=1)\n",
    "            dec_outputs.append(output_i)\n",
    "\n",
    "        dec_outputs = torch.stack(dec_outputs)\n",
    "        return dec_outputs\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, alphabet_size, batch_size, maxlen):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(alphabet_size, 512)\n",
    "\n",
    "    def forward(self, x, t, fmask, device,):\n",
    "        enc_out = self.encoder(x, fmask)\n",
    "        dec_out = self.decoder(t, enc_out, device=device)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_path, dev_path, aud_path, alphabet_path, model_path, maxlen, maxlent,\n",
    "          num_epochs=10,  batch_size=32, device_id=0):\n",
    "\n",
    "    print(\"Num epochs:\", num_epochs, \"Batch size:\", batch_size)\n",
    "\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "\n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.apply(weights)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = customNLLLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "\n",
    "    init_val_loss = 9999999\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_dataset = Data(train_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_loss = 0\n",
    "        loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        num_steps = len(loader)\n",
    "        step = 0\n",
    "        for batch in loader:\n",
    "            step+=1\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "            print(\"Step {}/{}. Loss: {:>4f}\".format(step, num_steps, loss.detach().cpu().numpy()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        losses.append(epoch_loss/len(loader))\n",
    "        np.save(os.path.join(model_path, 'train_loss.npy'), np.array(losses))\n",
    "        print('Epoch:{}/{} Training loss:{:>4f}'.format(epoch, num_epochs, epoch_loss/len(loader)))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        #Validation\n",
    "        dev_dataset = Data(dev_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "        val_loss = 0\n",
    "        loader = data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for batch in loader:\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "\n",
    "            val_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        curr_val_loss = val_loss/len(loader)\n",
    "        val_losses.append(curr_val_loss)\n",
    "        np.save(os.path.join(model_path, \"val_losses.npy\"), np.array(val_losses))\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        print('Epoch:{}/{} Validation loss:{:>4f}'.format(epoch, num_epochs, curr_val_loss))\n",
    "\n",
    "        ## Model Selection\n",
    "        if curr_val_loss < init_val_loss:\n",
    "            torch.save(best_model, os.path.join(model_path, \"model_best.pth\"))\n",
    "            init_val_loss = curr_val_loss\n",
    "        torch.save(best_model, os.path.join(model_path, \"model_last.pth\"))\n",
    "\n",
    "\n",
    "def predict(test_path, aud_path, alphabet_path, model_path, batch_size, maxlen, maxlent, device_id=0):\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "    ind2char = {char2ind[key]:key for key in char2ind}\n",
    "\n",
    "    ctc_decoder = CTCDecoder(alphabet)\n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"model_best.pth\")))\n",
    "    model = model.to(device)\n",
    "\n",
    "    test_dataset = Data(test_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    total_WER = 0\n",
    "    total_CER = 0\n",
    "    step = 0\n",
    "    num_steps = len(loader)\n",
    "\n",
    "    targets = []\n",
    "    predicted = []\n",
    "    \n",
    "    print(\"Total number of examples: \", num_steps*batch_size)\n",
    "    \n",
    "    for batch in loader:\n",
    "        step+=1\n",
    "        print(\"Decoding step {}/{}...\".format(step, num_steps))\n",
    "        batch_WER = 0\n",
    "        batch_CER = 0\n",
    "\n",
    "        x = batch['aud'].to(device)\n",
    "        t = batch['trans'].to(device)\n",
    "        fmask = batch['fmask'].squeeze(1).to(device)\n",
    "        tmask = batch['tmask'].squeeze(1).to(device)\n",
    "        preds = model(x, t, fmask, device)\n",
    "        preds = torch.transpose(preds, 0, 1)\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    " \n",
    "        tmask = tmask.detach().cpu().numpy()\n",
    "        for i, probs in enumerate(preds):\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            probs = np.exp(probs[:pad_ind,])\n",
    "            seq , _ = ctc_decoder.decode(probs, beam_size=5)\n",
    "            seq = ''.join([ind2char[ind] for ind in seq])\n",
    "            seq = collapse_fn(seq)\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            target = t[i][:pad_ind]\n",
    "            target = ''.join([ind2char[ind] for ind in target])\n",
    "            targets.append(target)\n",
    "            predicted.append(seq)\n",
    "\n",
    "    save_predictions(targets, predicted, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
