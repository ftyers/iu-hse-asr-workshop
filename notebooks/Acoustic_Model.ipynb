{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/clips/'\n",
    "train_df = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphabet \n",
    "with open('/Users/tacha/iu_research/speech_recognition/asr_workshop/iu-hse-asr-workshop/data/alphabet.txt', 'r') as fo:\n",
    "    alphabet = fo.readlines() + ['f', 'i', 'r', 'e', 'o', 'x']\n",
    "char2ind = {alphabet[i].strip():i for i in range(len(alphabet))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxlen(path, train_df):\n",
    "    fnames = pd.read_csv(train_df, sep='\\t')['path']\n",
    "    maxlen = 0\n",
    "    for n in tqdm(fnames):\n",
    "        waveform, sample_rate = torchaudio.load(os.path.join(path, n))\n",
    "        mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "        size = mfcc.shape[2]\n",
    "        if size > maxlen:\n",
    "            maxlen = size\n",
    "    print(\"Maxlen:\", maxlen)\n",
    "\n",
    "\n",
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(1, norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "\n",
    "def encode_trans(trans, char2ind, maxlen_t=7):\n",
    "    '''\n",
    "    Encodes true transcription\n",
    "    trans: \n",
    "    '''\n",
    "    res = np.array([char2ind[char] for char in trans])\n",
    "    res = np.pad(res, (0, maxlen_t-len(res)), 'constant', constant_values=(-1))\n",
    "    mask = [1 if i>=0 else 0 for i in res]\n",
    "    return torch.tensor(res), torch.tensor(mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, char2ind, transforms):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.char2ind = char2ind\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, fmask = self.transforms[0](fname)\n",
    "        trans, tmask = self.transforms[1](transcript, self.char2ind)\n",
    "        sample = {'aud':feat, 'trans': trans, 'fmask':fmask, 'tmask':tmask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize weights randomly\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed Architechture**\n",
    "\n",
    "Attention-based Sequence-to-Sequence model:\n",
    "\n",
    "![](/img/arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=3, \n",
    "                             bidirectional=True)\n",
    "        self.h0 = torch.zeros(3*2, batch_size, 256)\n",
    "        self.c0 = torch.zeros(3*2, batch_size, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Pass through the first linear layer\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        #Pass through LSTM layers\n",
    "        output, (hn, cn) = self.blstm(outputs, (self.h0, self.c0))\n",
    "        return output, (hn, cn)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, h_e, h_d):\n",
    "        score = torch.matmul(h_e.T, h_d)\n",
    "        temp1 = torch.exp(score)\n",
    "        temp2 = torch.sum(score, dim=0)\n",
    "        a_t = temp1/temp2\n",
    "        c_t = torch.zeros(h_e.shape)\n",
    "        for a in a_t:\n",
    "            c_t+=a*h_e  \n",
    "        return c_t\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.char2ind = char2ind\n",
    "        self.embed_layer = nn.Linear(33, 128)\n",
    "        self.lstm_cell = nn.LSTMCell(128, 512)\n",
    "        self.output = nn.Linear(512, 33)\n",
    "        self.dec_h = torch.zeros(batch_size, 512)\n",
    "        self.c = torch.zeros(batch_size, 512)\n",
    "        self.y = torch.zeros(batch_size,  33)\n",
    "        self.attention = Attention()\n",
    "    \n",
    "    def forward(self, enc_h):\n",
    "        preds = []\n",
    "        for hidden in enc_h:\n",
    "            c_t = self.attention(hidden, self.dec_h)\n",
    "            y = self.embed_layer(self.y)\n",
    "            self.dec_h, self.c = self.lstm_cell(y, (self.dec_h, self.c))\n",
    "            self.y = self.output(self.dec_h)\n",
    "            y_hat = nn.functional.log_softmax(self.y, dim=1)\n",
    "            preds.append(y_hat)\n",
    "        preds = torch.stack(preds)\n",
    "        return preds\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(batch_size)\n",
    "        self.decoder = Decoder(batch_size)\n",
    "    def forward(self, batch):\n",
    "        enc_out, (he, ce) = self.encoder(batch)\n",
    "        preds = self.decoder(enc_out)\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "def collapse_fn(preds, masks):\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    masks = masks.detach().cpu().numpy()\n",
    "    collapsed = []\n",
    "    maxlen_t = 0\n",
    "    for pred, mask in zip(preds, masks):\n",
    "        temp = [pred[0]]\n",
    "        for i, char in enumerate(pred[1:]):\n",
    "            if mask[i]:\n",
    "                if pred[i-1]==char:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp.append(char)\n",
    "        collapsed.append(temp)\n",
    "        maxlen_t = max(maxlen_t, len(temp))\n",
    "    \n",
    "    res = []\n",
    "    for sent in collapsed:\n",
    "        sent = np.pad(sent, (0, maxlen_t - len(sent)), 'constant', constant_values=(-1))\n",
    "        res.append(sent)\n",
    "        \n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(32)\n",
    "model.apply(weights)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CTCLoss(zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "cv_dataset = TrainData(train_df, train_aud, char2ind, [extract_feats, encode_trans])\n",
    "loader = data.DataLoader(cv_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "for batch in loader:\n",
    "    x = batch['aud'].to(device)\n",
    "    t = batch['trans'].to(device)\n",
    "    fmask = batch['fmask'].squeeze(1)\n",
    "    tmask = batch['tmask'].squeeze(1)\n",
    "    preds = model(x)\n",
    "    input_length = torch.sum(fmask, dim =1).long()\n",
    "    target_length = torch.sum(tmask, dim=1).long()\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds, t, input_length, target_length)\n",
    "    print(loss.detach().cpu().numpy())\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(\"----------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
