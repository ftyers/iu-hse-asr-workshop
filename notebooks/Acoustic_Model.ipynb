{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Architechture\n",
    "<img src=\"../img/arch.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "**Encoder:** bidirectional LSTM with 3 layers, 512 hidden units.\n",
    "\n",
    "**Decoder:** unidirectional LSTM with 1 layer and 512 hidden units.\n",
    "\n",
    "**Attention:** module is a solution for bottleneck problem. Helps to retrieve relevant information from all of the time step of the input.\n",
    "\n",
    "$$Score=\\langle H_s^E, H_t^D \\rangle$$\n",
    "\n",
    "$$Align = \\frac{exp\\{Score(H_s^E, H_t^D)\\}}{\\Sigma_{s=1}^S exp\\{Score(H_s^E, H_t^D)\\}}$$\n",
    "\n",
    "<img src=\"../img/attn.gif\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "**Context vector:** $$c_t = \\Sigma_{s=1}^S a_t(s)*H_s^E$$\n",
    "\n",
    "\n",
    "\n",
    "-------\n",
    "**Reference:**\n",
    "1. [Tjandra et al. 2018](https://arxiv.org/pdf/1710.10774.pdf)\n",
    "2. [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) on LTSMs.\n",
    "3. [S. Nadig. Attention in end-to-end Automatic Speech Recognition.](https://medium.com/intel-student-ambassadors/attention-in-end-to-end-automatic-speech-recognition-9f9e42718d21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(1, norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "\n",
    "def encode_trans(trans, char2ind, maxlen_t=7):\n",
    "    '''\n",
    "    Encodes true transcription\n",
    "    trans: \n",
    "    '''\n",
    "    res = np.array([char2ind[char] for char in trans])\n",
    "    res = np.pad(res, (0, maxlen_t-len(res)), 'constant', constant_values=(0))\n",
    "    mask = [1 if i>0 else 0 for i in res]\n",
    "    return torch.tensor(res), torch.tensor(mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, char2ind, transforms, maxlen, maxlent):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.char2ind = char2ind\n",
    "        self.transforms = transforms\n",
    "        self.maxlen = maxlen\n",
    "        self.maxlent = maxlent\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, fmask = self.transforms[0](fname, self.maxlen)\n",
    "        trans, tmask = self.transforms[1](transcript, self.char2ind, self.maxlent)\n",
    "        sample = {'aud': nan_to_num(feat), 'trans': trans, 'fmask':fmask, 'tmask':tmask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize weights randomly\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)\n",
    "\n",
    "def nan_to_num(t,mynan=0.):\n",
    "    if torch.all(torch.isfinite(t)):\n",
    "        return t\n",
    "    if len(t.size()) == 0:\n",
    "        return torch.tensor(mynan)\n",
    "    return torch.cat([nan_to_num(l).unsqueeze(0) for l in t],0)\n",
    "\n",
    "def save_predictions(target, predicted, model_path):\n",
    "    path = os.path.join(model_path, \"predicted.txt\")\n",
    "    with open(path, 'w') as fo:\n",
    "        for i in range(len(target)):\n",
    "            fo.write(target[i] + \"|\" + predicted[i] + '\\n')\n",
    "            \n",
    "def collapse_fn(preds):\n",
    "    seq = ''\n",
    "    prev = ''\n",
    "    for char in preds:\n",
    "        if not prev:\n",
    "            prev = char\n",
    "            seq+=char\n",
    "        if char==prev:\n",
    "            continue\n",
    "        else:\n",
    "            prev=char\n",
    "            seq+=char\n",
    "    return seq\n",
    "\n",
    "class customNLLLoss(nn.Module):\n",
    "    '''\n",
    "    Negative Log Likelihood Loss\n",
    "    '''\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        if self.ignore_index:\n",
    "            self.loss = nn.NLLLoss(ignore_index=self.ignore_index)\n",
    "        else:\n",
    "            self.loss = nn.NLLLoss()\n",
    "    def forward(self, inp, target):\n",
    "        loss = 0\n",
    "        for i, char in enumerate(inp):\n",
    "            loss+=self.loss(char,target[:,i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=2,\n",
    "                             dropout=0.3, \n",
    "                             bidirectional=True)\n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            out = self.drop(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        lengths = torch.sum(mask, dim=1).detach().cpu()\n",
    "        outputs = pack_padded_sequence(outputs, lengths, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.blstm(outputs)\n",
    "        output, _ = pad_packed_sequence(output, total_length=mask.shape[1])\n",
    "        return output\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, enc_hid_states, dec_hid, device):\n",
    "        enc_hid_states = torch.transpose(enc_hid_states, 0, 1)\n",
    "        scores = torch.zeros(dec_hid.shape[0], enc_hid_states.shape[0]).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            score_i = torch.bmm(dec_hid.unsqueeze(1), enc_hid.unsqueeze(2))[:,0,0]\n",
    "            scores[:, i] = score_i\n",
    "        \n",
    "        align = F.softmax(scores, dim=1)\n",
    "        c_t = torch.zeros(dec_hid.shape).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            c_t+= align[:, i].unsqueeze(1)*enc_hid\n",
    "        return c_t\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(output_size, 128)\n",
    "        #self.lstm_cell = nn.LSTMCell(128, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=128, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=1,\n",
    "                            dropout=0.3)\n",
    "        self.output = nn.Linear(2* hidden_size, output_size)\n",
    "        self.attention = Attention()\n",
    "        self.drop_lstm = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, target_inputs, encoder_outputs, device=None):\n",
    "        dec_hid = encoder_outputs[-1].unsqueeze(0)\n",
    "\n",
    "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1)\n",
    "        c_i = torch.zeros(dec_hid.shape).to(device)\n",
    "        dec_outputs = []\n",
    "\n",
    "        for inp in torch.transpose(target_inputs, 0, 1):\n",
    "            embedded = self.embed_layer(inp)\n",
    "            dec_out, (dec_hid, _) = self.lstm(embedded.unsqueeze(0), (dec_hid, c_i))\n",
    "            context = self.attention(encoder_outputs, dec_out.squeeze(0), device)\n",
    "            combined_input = torch.cat([dec_hid.squeeze(0), context], 1)\n",
    "            output_i = self.output(combined_input)\n",
    "            output_i = F.log_softmax(output_i, dim=1)\n",
    "            dec_outputs.append(output_i)\n",
    "\n",
    "        dec_outputs = torch.stack(dec_outputs)\n",
    "        return dec_outputs\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, alphabet_size, batch_size, maxlen):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(alphabet_size, 512)\n",
    "\n",
    "    def forward(self, x, t, fmask, device,):\n",
    "        enc_out = self.encoder(x, fmask)\n",
    "        dec_out = self.decoder(t, enc_out, device=device)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_path, dev_path, aud_path, alphabet_path, model_path, maxlen, maxlent,\n",
    "          num_epochs=10,  batch_size=32, device_id=0):\n",
    "\n",
    "    print(\"Num epochs:\", num_epochs, \"Batch size:\", batch_size)\n",
    "\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "\n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.apply(weights)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = customNLLLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "\n",
    "    init_val_loss = 9999999\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_dataset = Data(train_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_loss = 0\n",
    "        loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        num_steps = len(loader)\n",
    "        step = 0\n",
    "        for batch in loader:\n",
    "            step+=1\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "            print(\"Step {}/{}. Loss: {:>4f}\".format(step, num_steps, loss.detach().cpu().numpy()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        losses.append(epoch_loss/len(loader))\n",
    "        np.save(os.path.join(model_path, 'train_loss.npy'), np.array(losses))\n",
    "        print('Epoch:{}/{} Training loss:{:>4f}'.format(epoch, num_epochs, epoch_loss/len(loader)))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        #Validation\n",
    "        dev_dataset = Data(dev_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "        val_loss = 0\n",
    "        loader = data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for batch in loader:\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "\n",
    "            val_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        curr_val_loss = val_loss/len(loader)\n",
    "        val_losses.append(curr_val_loss)\n",
    "        np.save(os.path.join(model_path, \"val_losses.npy\"), np.array(val_losses))\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        print('Epoch:{}/{} Validation loss:{:>4f}'.format(epoch, num_epochs, curr_val_loss))\n",
    "\n",
    "        ## Model Selection\n",
    "        if curr_val_loss < init_val_loss:\n",
    "            torch.save(best_model, os.path.join(model_path, \"model_best.pth\"))\n",
    "            init_val_loss = curr_val_loss\n",
    "        torch.save(best_model, os.path.join(model_path, \"model_last.pth\"))\n",
    "\n",
    "\n",
    "def predict(test_path, aud_path, alphabet_path, model_path, batch_size, maxlen, maxlent, device_id=0):\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "    ind2char = {char2ind[key]:key for key in char2ind}\n",
    "\n",
    "    ctc_decoder = CTCDecoder(alphabet)\n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"model_best.pth\")))\n",
    "    model = model.to(device)\n",
    "\n",
    "    test_dataset = Data(test_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    step = 0\n",
    "    num_steps = len(loader)\n",
    "\n",
    "    targets = []\n",
    "    predicted = []\n",
    "    \n",
    "    print(\"Total number of examples: \", num_steps*batch_size)\n",
    "    \n",
    "    for batch in loader:\n",
    "        step+=1\n",
    "        print(\"Decoding step {}/{}...\".format(step, num_steps))\n",
    "        batch_WER = 0\n",
    "        batch_CER = 0\n",
    "\n",
    "        x = batch['aud'].to(device)\n",
    "        t = batch['trans'].to(device)\n",
    "        fmask = batch['fmask'].squeeze(1).to(device)\n",
    "        tmask = batch['tmask'].squeeze(1).to(device)\n",
    "        preds = model(x, t, fmask, device)\n",
    "        preds = torch.transpose(preds, 0, 1)\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    " \n",
    "        tmask = tmask.detach().cpu().numpy()\n",
    "        for i, probs in enumerate(preds):\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            probs = np.exp(probs[:pad_ind,])\n",
    "            seq , _ = ctc_decoder.decode(probs, beam_size=5)\n",
    "            seq = ''.join([ind2char[ind] for ind in seq])\n",
    "            seq = collapse_fn(seq)\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            target = t[i][:pad_ind]\n",
    "            target = ''.join([ind2char[ind] for ind in target])\n",
    "            targets.append(target)\n",
    "            predicted.append(seq)\n",
    "\n",
    "    save_predictions(targets, predicted, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/train.tsv'\n",
    "DEV = '../data/dev.tsv'\n",
    "AUD_PATH = '../data/ru/clips/'\n",
    "ALPHABET = '../data/alphabet.txt'\n",
    "MODEL = ''\n",
    "MAXLEN = 1083\n",
    "MAXLENT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num epochs: 10 Batch size: 32\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torchaudio/functional.py:317: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1603740477510/work/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1603740477510/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/17. Loss: 67.187958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f34e3b9c89a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUD_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALPHABET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXLENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-22e8c6bac6f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_path, dev_path, aud_path, alphabet_path, model_path, maxlen, maxlent, num_epochs, batch_size, device_id)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step {}/{}. Loss: {:>4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prog/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prog/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(TRAIN, DEV, AUD_PATH, ALPHABET, MODEL, MAXLEN, MAXLENT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
