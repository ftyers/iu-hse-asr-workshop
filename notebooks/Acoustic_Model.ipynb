{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/clips/'\n",
    "train_df = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphabet \n",
    "with open('/Users/tacha/iu_research/speech_recognition/asr_workshop/iu-hse-asr-workshop/data/alphabet.txt', 'r') as fo:\n",
    "    alphabet = fo.readlines() + ['f', 'i', 'r', 'e', 'o', 'x']\n",
    "char2ind = {alphabet[i].strip():i for i in range(len(alphabet))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxlen(path, train_df):\n",
    "    fnames = pd.read_csv(train_df, sep='\\t')['path']\n",
    "    maxlen = 0\n",
    "    for n in tqdm(fnames):\n",
    "        waveform, sample_rate = torchaudio.load(os.path.join(path, n))\n",
    "        mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "        size = mfcc.shape[2]\n",
    "        if size > maxlen:\n",
    "            maxlen = size\n",
    "    print(\"Maxlen:\", maxlen)\n",
    "\n",
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(1, norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "\n",
    "def encode_trans(trans, char2ind, maxlen_t=7):\n",
    "    '''\n",
    "    Encodes true transcription\n",
    "    trans: \n",
    "    '''\n",
    "    res = np.array([char2ind[char] for char in trans])\n",
    "    res = np.pad(res, (0, maxlen_t-len(res)), 'constant', constant_values=(-1))\n",
    "    mask = [1 if i>=0 else 0 for i in res]\n",
    "    return torch.tensor(res), torch.tensor(mask) \n",
    "\n",
    "\n",
    "def collapse_fn(preds, masks):\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    masks = masks.detach().cpu().numpy()\n",
    "    collapsed = []\n",
    "    maxlen_t = 0\n",
    "    for pred, mask in zip(preds, masks):\n",
    "        temp = [pred[0]]\n",
    "        for i, char in enumerate(pred[1:]):\n",
    "            if mask[i]:\n",
    "                if pred[i-1]==char:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp.append(char)\n",
    "        collapsed.append(temp)\n",
    "        maxlen_t = max(maxlen_t, len(temp))\n",
    "    \n",
    "    res = []\n",
    "    for sent in collapsed:\n",
    "        sent = np.pad(sent, (0, maxlen_t - len(sent)), 'constant', constant_values=(-1))\n",
    "        res.append(sent)\n",
    "        \n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, char2ind, transforms):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.char2ind = char2ind\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, fmask = self.transforms[0](fname)\n",
    "        trans, tmask = self.transforms[1](transcript, self.char2ind)\n",
    "        sample = {'aud': nan_to_num(feat), 'trans': trans, 'fmask':fmask, 'tmask':tmask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize weights randomly\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)\n",
    "\n",
    "def nan_to_num(t,mynan=0.):\n",
    "    if torch.all(torch.isfinite(t)):\n",
    "        return t\n",
    "    if len(t.size()) == 0:\n",
    "        return torch.tensor(mynan)\n",
    "    return torch.cat([nan_to_num(l).unsqueeze(0) for l in t],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=3, \n",
    "                             bidirectional=True)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        lengths = torch.sum(mask, dim=1).detach().cpu()\n",
    "        outputs = pack_padded_sequence(outputs, lengths, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.blstm(outputs)\n",
    "        output, _ = pad_packed_sequence(output, total_length=mask.shape[1])\n",
    "        return output, (hn, cn)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, h_e, h_d):\n",
    "        score = torch.matmul(h_e.T, h_d)\n",
    "        a_t = nn.functional.softmax(score, dim=0)\n",
    "        c_t = torch.sum(a_t, dim=0)*h_e \n",
    "        return c_t\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Linear(33, 128)\n",
    "        self.lstm_cell = nn.LSTMCell(128, 512)\n",
    "        self.output = nn.Linear(1024, 33)\n",
    "        self.attention = Attention()\n",
    "        self.dec_h = None \n",
    "        self.dec_c = None\n",
    "\n",
    "    def forward(self, enc_h, y):\n",
    "        preds = []\n",
    "        for i, hidden in enumerate(enc_h):\n",
    "            if i==0:\n",
    "                self.dec_h, self.dec_c = self.lstm_cell(y)\n",
    "            else:\n",
    "                self.dec_h, self.dec_c = self.lstm_cell(y, (self.dec_h, self.dec_c))\n",
    "            c_t = self.attention(hidden, self.dec_h)\n",
    "            combined_input = torch.cat([self.dec_h, c_t], 1)\n",
    "            y_hat = self.output(combined_input)\n",
    "            \n",
    "            output = nn.functional.log_softmax(y_hat, dim=1)\n",
    "            y = self.embed_layer(y_hat)\n",
    "            preds.append(output)\n",
    "        preds = torch.stack(preds)\n",
    "        return preds\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x, mask, dec_input):\n",
    "        enc_out, (he, ce) = self.encoder(x, mask)\n",
    "        preds = self.decoder(enc_out, dec_input)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(32)\n",
    "model.apply(weights)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CTCLoss(zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "cv_dataset = TrainData(train_df, train_aud, char2ind, [extract_feats, encode_trans])\n",
    "loader = data.DataLoader(cv_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(csv_path, aud_path, alphabet_path, num_epochs=10,  batch_size=32, enc_hidden_size=256):\n",
    "\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = fo.readlines() + ['f', 'i', 'r', 'e', 'o', 'x']\n",
    "    char2ind = {alphabet[i].strip():i for i in range(len(alphabet))}\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq()\n",
    "    model.apply(weights)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CTCLoss(zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    cv_dataset = TrainData(csv_path, aud_path, char2ind, [extract_feats, encode_trans])\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_loss = 0\n",
    "        loader = data.DataLoader(cv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        for batch in loader:\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            dec_input = torch.randn(x.shape[0], 128, requires_grad=True).to(device)\n",
    "\n",
    "            preds = model(x, fmask, dec_input)\n",
    "            input_length = torch.sum(fmask, dim =1).long().to(device)\n",
    "            target_length = torch.sum(tmask, dim=1).long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(preds, t, input_length, target_length)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.detach().cpu().numpy()\n",
    "        print('Epoch:{:3}/{:3} Training loss:{:>4f}'.format(epoch, num_epochs, epoch_loss/len(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
