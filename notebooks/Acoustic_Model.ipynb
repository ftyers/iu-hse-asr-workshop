{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fran/.local/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/tmp/target-segments/ru/clips/'\n",
    "train_df = '/tmp/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxlen(path, train_df):\n",
    "    fnames = pd.read_csv(train_df, sep='\\t')['path']\n",
    "    maxlen = 0\n",
    "    for n in tqdm(fnames):\n",
    "        waveform, sample_rate = torchaudio.load(os.path.join(path, n))\n",
    "        mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "        size = mfcc.shape[2]\n",
    "        if size > maxlen:\n",
    "            maxlen = size\n",
    "    print(\"Maxlen:\", maxlen)\n",
    "\n",
    "\n",
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(norm.shape[0], norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "def alphabet_enc(csv_path):\n",
    "    char2ind = {}\n",
    "    sents = pd.read_csv(csv_path, sep='\\t')['sentence']\n",
    "    chars = list(set([char for sent in sents for char in sent]))\n",
    "    for i in range(len(chars)):\n",
    "        char2ind[chars[i]] = i\n",
    "    char2ind[\"<eos>\"] = len(chars)+1 \n",
    "    return char2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, transform):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, mask = self.transform(fname)\n",
    "\n",
    "        sample = {'aud':feat, 'trans': transcript, 'mask':mask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize random weights\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed Architechture**\n",
    "\n",
    "Attention-based Sequence-to-Sequence model:\n",
    "\n",
    "![](/img/arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=3, \n",
    "                             bidirectional=True)\n",
    "        self.h0 = torch.zeros(3*2, batch_size, 256)\n",
    "        self.c0 = torch.zeros(3*2, batch_size, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Pass through the first linear layer\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        #Pass through LSTM layers\n",
    "        output, (hn, cn) = self.blstm(outputs, (self.h0, self.c0))\n",
    "        return output, (hn, cn)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size, char2ind):\n",
    "        super().__init__()\n",
    "        self.char2ind = char2ind\n",
    "        self.embed_layer = nn.Linear(512, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=512, \n",
    "                             num_layers=1)\n",
    "        self.h0 = torch.zeros(1, batch_size, 512)\n",
    "        self.c0 = torch.zeros(1, batch_size, 512)\n",
    "        self.y0 = torch.zeros(1, 512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2ind = alphabet_enc(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(32)\n",
    "encoder = encoder.to(device)\n",
    "encoder.apply(weights)\n",
    "\n",
    "cv_dataset = TrainData(train_df, train_aud, extract_feats)\n",
    "loader = data.DataLoader(cv_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(32, char2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fran/.local/lib/python3.8/site-packages/torchaudio/functional.py:317: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/fran/.local/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/home/fran/.local/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.5850e-03,  4.8217e-03, -4.0246e-03,  ...,  1.2783e-02,\n",
      "          -3.6178e-02,  8.2143e-04],\n",
      "         [ 5.2252e-03,  3.5812e-03, -7.4774e-03,  ...,  4.8239e-02,\n",
      "          -4.2366e-02, -6.0497e-03],\n",
      "         [-1.8553e-03,  4.5822e-03, -6.5938e-03,  ...,  3.4576e-02,\n",
      "          -3.8906e-02, -1.0214e-02],\n",
      "         ...,\n",
      "         [ 9.9936e-03,  2.5506e-03, -6.3852e-03,  ...,  2.8113e-02,\n",
      "          -4.5586e-02, -2.9782e-02],\n",
      "         [-3.6925e-04,  8.0831e-03, -2.8792e-03,  ...,  2.1298e-02,\n",
      "          -4.1732e-02, -4.5197e-03],\n",
      "         [ 3.9447e-03,  3.7340e-03,  3.4174e-04,  ...,  2.5715e-02,\n",
      "          -3.9151e-02, -2.1908e-02]],\n",
      "\n",
      "        [[-5.8371e-03,  4.7725e-03, -5.7921e-03,  ...,  8.9470e-03,\n",
      "          -3.6790e-02,  1.9270e-03],\n",
      "         [ 1.0485e-02,  2.1323e-03, -1.1960e-02,  ...,  5.8033e-02,\n",
      "          -4.3373e-02, -4.0174e-03],\n",
      "         [-1.3855e-03,  5.2263e-03, -1.0116e-02,  ...,  3.8426e-02,\n",
      "          -3.7583e-02, -1.0563e-02],\n",
      "         ...,\n",
      "         [ 2.0178e-02,  9.9949e-04, -9.5034e-03,  ...,  2.7161e-02,\n",
      "          -4.7804e-02, -3.4982e-02],\n",
      "         [ 2.6340e-04,  1.1123e-02, -3.5240e-03,  ...,  2.1436e-02,\n",
      "          -4.3512e-02, -4.2869e-03],\n",
      "         [ 1.0535e-02,  2.3631e-03,  4.2096e-03,  ...,  2.4717e-02,\n",
      "          -3.4464e-02, -2.5858e-02]],\n",
      "\n",
      "        [[-7.4358e-03,  4.0146e-03, -6.2760e-03,  ...,  6.2713e-03,\n",
      "          -3.6610e-02,  2.6192e-03],\n",
      "         [ 1.4745e-02, -2.5626e-04, -1.5155e-02,  ...,  6.9620e-02,\n",
      "          -4.3455e-02, -8.2352e-05],\n",
      "         [ 1.0065e-03,  6.7426e-03, -1.1284e-02,  ...,  4.2919e-02,\n",
      "          -3.3534e-02, -9.4661e-03],\n",
      "         ...,\n",
      "         [ 3.0509e-02,  5.4111e-04, -1.0211e-02,  ...,  2.5941e-02,\n",
      "          -4.7766e-02, -3.8220e-02],\n",
      "         [ 1.5002e-03,  1.4234e-02, -2.6208e-03,  ...,  2.3975e-02,\n",
      "          -4.4213e-02, -4.6352e-03],\n",
      "         [ 1.9369e-02,  1.2846e-03,  9.7762e-03,  ...,  2.4473e-02,\n",
      "          -2.4182e-02, -2.8432e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03],\n",
      "         [-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03],\n",
      "         [-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03],\n",
      "         ...,\n",
      "         [-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03],\n",
      "         [-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03],\n",
      "         [-1.2947e-02,  4.7755e-03, -6.4089e-03,  ..., -8.7466e-03,\n",
      "          -2.5904e-02,  5.6317e-03]],\n",
      "\n",
      "        [[-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03],\n",
      "         [-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03],\n",
      "         [-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03],\n",
      "         ...,\n",
      "         [-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03],\n",
      "         [-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03],\n",
      "         [-1.3671e-02,  5.1271e-03, -6.8798e-03,  ..., -1.1399e-02,\n",
      "          -2.0202e-02,  5.6375e-03]],\n",
      "\n",
      "        [[-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03],\n",
      "         [-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03],\n",
      "         [-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03],\n",
      "         ...,\n",
      "         [-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03],\n",
      "         [-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03],\n",
      "         [-1.3910e-02,  5.3271e-03, -7.1863e-03,  ..., -1.0971e-02,\n",
      "          -1.1670e-02,  4.7042e-03]]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    x = batch['aud'].to(device)\n",
    "    out, (h, c) = encoder(x)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
