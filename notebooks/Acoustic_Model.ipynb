{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tacha/miniconda3/envs/prog/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/clips/'\n",
    "train_df = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Architechture\n",
    "<img src=\"../img/arch.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "**Encoder:** bidirectional LSTM with 3 layers, 512 hidden units.\n",
    "\n",
    "**Decoder:** unidirectional LSTM with 1 layer and 512 hidden units.\n",
    "\n",
    "**Attention:** module is a solution for bottleneck problem. Helps to retrieve relevant information from all of the time step of the input.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------\n",
    "**Reference:**\n",
    "1. [Tjandra et al. 2018](https://arxiv.org/pdf/1710.10774.pdf)\n",
    "2. [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) on LTSMs.\n",
    "3. [S. Nadig. Attention in end-to-end Automatic Speech Recognition.](https://medium.com/intel-student-ambassadors/attention-in-end-to-end-automatic-speech-recognition-9f9e42718d21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(1, norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "\n",
    "def encode_trans(trans, char2ind, maxlen_t=7):\n",
    "    '''\n",
    "    Encodes true transcription\n",
    "    trans: \n",
    "    '''\n",
    "    res = np.array([char2ind[char] for char in trans])\n",
    "    res = np.pad(res, (0, maxlen_t-len(res)), 'constant', constant_values=(0))\n",
    "    mask = [1 if i>0 else 0 for i in res]\n",
    "    return torch.tensor(res), torch.tensor(mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, char2ind, transforms, maxlen, maxlent):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.char2ind = char2ind\n",
    "        self.transforms = transforms\n",
    "        self.maxlen = maxlen\n",
    "        self.maxlent = maxlent\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, fmask = self.transforms[0](fname, self.maxlen)\n",
    "        trans, tmask = self.transforms[1](transcript, self.char2ind, self.maxlent)\n",
    "        sample = {'aud': nan_to_num(feat), 'trans': trans, 'fmask':fmask, 'tmask':tmask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize weights randomly\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)\n",
    "\n",
    "def nan_to_num(t,mynan=0.):\n",
    "    if torch.all(torch.isfinite(t)):\n",
    "        return t\n",
    "    if len(t.size()) == 0:\n",
    "        return torch.tensor(mynan)\n",
    "    return torch.cat([nan_to_num(l).unsqueeze(0) for l in t],0)\n",
    "\n",
    "def save_predictions(target, predicted, model_path):\n",
    "    path = os.path.join(model_path, \"predicted.txt\")\n",
    "    with open(path, 'w') as fo:\n",
    "        for i in range(len(target)):\n",
    "            fo.write(target[i] + \"|\" + predicted[i] + '\\n')\n",
    "            \n",
    "def collapse_fn(preds):\n",
    "    seq = ''\n",
    "    prev = ''\n",
    "    for char in preds:\n",
    "        if not prev:\n",
    "            prev = char\n",
    "            seq+=char\n",
    "        if char==prev:\n",
    "            continue\n",
    "        else:\n",
    "            prev=char\n",
    "            seq+=char\n",
    "    return seq\n",
    "\n",
    "class customNLLLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        if self.ignore_index:\n",
    "            self.loss = nn.NLLLoss(ignore_index=self.ignore_index)\n",
    "        else:\n",
    "            self.loss = nn.NLLLoss()\n",
    "    def forward(self, inp, target):\n",
    "        loss = 0\n",
    "        for i, char in enumerate(inp):\n",
    "            loss+=self.loss(char,target[:,i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=2,\n",
    "                             dropout=0.3, \n",
    "                             bidirectional=True)\n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            out = self.drop(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        lengths = torch.sum(mask, dim=1).detach().cpu()\n",
    "        outputs = pack_padded_sequence(outputs, lengths, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.blstm(outputs)\n",
    "        output, _ = pad_packed_sequence(output, total_length=mask.shape[1])\n",
    "        return output\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, enc_hid_states, dec_hid, device):\n",
    "        enc_hid_states = torch.transpose(enc_hid_states, 0, 1)\n",
    "        scores = torch.zeros(dec_hid.shape[0], enc_hid_states.shape[0]).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            score_i = torch.bmm(dec_hid.unsqueeze(1), enc_hid.unsqueeze(2))[:,0,0]\n",
    "            scores[:, i] = score_i\n",
    "        \n",
    "        align = F.softmax(scores, dim=1)\n",
    "        c_t = torch.zeros(dec_hid.shape).to(device)\n",
    "        for i, enc_hid in enumerate(enc_hid_states):\n",
    "            c_t+= align[:, i].unsqueeze(1)*enc_hid\n",
    "        return c_t\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(output_size, 128)\n",
    "        #self.lstm_cell = nn.LSTMCell(128, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=128, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=1,\n",
    "                            dropout=0.3)\n",
    "        self.output = nn.Linear(2* hidden_size, output_size)\n",
    "        self.attention = Attention()\n",
    "        self.drop_lstm = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, target_inputs, encoder_outputs, device=None):\n",
    "        dec_hid = encoder_outputs[-1].unsqueeze(0)\n",
    "\n",
    "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1)\n",
    "        c_i = torch.zeros(dec_hid.shape).to(device)\n",
    "        dec_outputs = []\n",
    "\n",
    "        for inp in torch.transpose(target_inputs, 0, 1):\n",
    "            embedded = self.embed_layer(inp)\n",
    "            dec_out, (dec_hid, _) = self.lstm(embedded.unsqueeze(0), (dec_hid, c_i))\n",
    "            context = self.attention(encoder_outputs, dec_out.squeeze(0), device)\n",
    "            combined_input = torch.cat([dec_hid.squeeze(0), context], 1)\n",
    "            output_i = self.output(combined_input)\n",
    "            output_i = F.log_softmax(output_i, dim=1)\n",
    "            dec_outputs.append(output_i)\n",
    "\n",
    "        dec_outputs = torch.stack(dec_outputs)\n",
    "        return dec_outputs\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, alphabet_size, batch_size, maxlen):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(alphabet_size, 512)\n",
    "\n",
    "    def forward(self, x, t, fmask, device,):\n",
    "        enc_out = self.encoder(x, fmask)\n",
    "        dec_out = self.decoder(t, enc_out, device=device)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_path, dev_path, aud_path, alphabet_path, model_path, maxlen, maxlent,\n",
    "          num_epochs=10,  batch_size=32, device_id=0):\n",
    "\n",
    "    print(\"Num epochs:\", num_epochs, \"Batch size:\", batch_size)\n",
    "\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "\n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.apply(weights)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = customNLLLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "\n",
    "    init_val_loss = 9999999\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_dataset = Data(train_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_loss = 0\n",
    "        loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        num_steps = len(loader)\n",
    "        step = 0\n",
    "        for batch in loader:\n",
    "            step+=1\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "            print(\"Step {}/{}. Loss: {:>4f}\".format(step, num_steps, loss.detach().cpu().numpy()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        losses.append(epoch_loss/len(loader))\n",
    "        np.save(os.path.join(model_path, 'train_loss.npy'), np.array(losses))\n",
    "        print('Epoch:{}/{} Training loss:{:>4f}'.format(epoch, num_epochs, epoch_loss/len(loader)))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        #Validation\n",
    "        dev_dataset = Data(dev_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "        val_loss = 0\n",
    "        loader = data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for batch in loader:\n",
    "            x = batch['aud'].to(device)\n",
    "            t = batch['trans'].to(device)\n",
    "            fmask = batch['fmask'].squeeze(1).to(device)\n",
    "            tmask = batch['tmask'].squeeze(1).to(device)\n",
    "            \n",
    "            model_out = model(x, t, fmask, device)\n",
    "    \n",
    "            loss = criterion(model_out, t)\n",
    "\n",
    "            val_loss+=loss.detach().cpu().numpy()\n",
    "\n",
    "        curr_val_loss = val_loss/len(loader)\n",
    "        val_losses.append(curr_val_loss)\n",
    "        np.save(os.path.join(model_path, \"val_losses.npy\"), np.array(val_losses))\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "        print('Epoch:{}/{} Validation loss:{:>4f}'.format(epoch, num_epochs, curr_val_loss))\n",
    "\n",
    "        ## Model Selection\n",
    "        if curr_val_loss < init_val_loss:\n",
    "            torch.save(best_model, os.path.join(model_path, \"model_best.pth\"))\n",
    "            init_val_loss = curr_val_loss\n",
    "        torch.save(best_model, os.path.join(model_path, \"model_last.pth\"))\n",
    "\n",
    "\n",
    "def predict(test_path, aud_path, alphabet_path, model_path, batch_size, maxlen, maxlent, device_id=0):\n",
    "    with open(alphabet_path, 'r') as fo:\n",
    "        alphabet = ['<pad>'] + fo.readlines()\n",
    "\n",
    "    char2ind = {alphabet[i].replace('\\n', ''):i for i in range(len(alphabet))}\n",
    "    ind2char = {char2ind[key]:key for key in char2ind}\n",
    "\n",
    "    ctc_decoder = CTCDecoder(alphabet)\n",
    "    \n",
    "    device = torch.device(\"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2Seq(alphabet_size=len(alphabet), batch_size=batch_size, maxlen=maxlen)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, \"model_best.pth\")))\n",
    "    model = model.to(device)\n",
    "\n",
    "    test_dataset = Data(test_path, aud_path, char2ind, [extract_feats, encode_trans], maxlen, maxlent)\n",
    "    loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    total_WER = 0\n",
    "    total_CER = 0\n",
    "    step = 0\n",
    "    num_steps = len(loader)\n",
    "\n",
    "    targets = []\n",
    "    predicted = []\n",
    "    \n",
    "    print(\"Total number of examples: \", num_steps*batch_size)\n",
    "    \n",
    "    for batch in loader:\n",
    "        step+=1\n",
    "        print(\"Decoding step {}/{}...\".format(step, num_steps))\n",
    "        batch_WER = 0\n",
    "        batch_CER = 0\n",
    "\n",
    "        x = batch['aud'].to(device)\n",
    "        t = batch['trans'].to(device)\n",
    "        fmask = batch['fmask'].squeeze(1).to(device)\n",
    "        tmask = batch['tmask'].squeeze(1).to(device)\n",
    "        preds = model(x, t, fmask, device)\n",
    "        preds = torch.transpose(preds, 0, 1)\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    " \n",
    "        tmask = tmask.detach().cpu().numpy()\n",
    "        for i, probs in enumerate(preds):\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            probs = np.exp(probs[:pad_ind,])\n",
    "            seq , _ = ctc_decoder.decode(probs, beam_size=5)\n",
    "            seq = ''.join([ind2char[ind] for ind in seq])\n",
    "            seq = collapse_fn(seq)\n",
    "            pad_ind = int(np.sum(tmask[i]))\n",
    "            target = t[i][:pad_ind]\n",
    "            target = ''.join([ind2char[ind] for ind in target])\n",
    "            targets.append(target)\n",
    "            predicted.append(seq)\n",
    "\n",
    "    save_predictions(targets, predicted, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
