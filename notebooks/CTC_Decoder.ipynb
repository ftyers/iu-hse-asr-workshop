{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code originally from [this gist](https://gist.github.com/awni/56369a90d03953e370f3964c826ed4b0).\n",
    "\n",
    "We start out by doing the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCDecoder:\n",
    "    \n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "        self.NEG_INF = -float(\"inf\")\n",
    "        self.trace = False\n",
    "    \n",
    "    def make_new_beam(self):\n",
    "        fn = lambda : (self.NEG_INF, self.NEG_INF)\n",
    "        return collections.defaultdict(fn)\n",
    "    \n",
    "    def logsumexp(self, *args):\n",
    "        \"\"\"\n",
    "        Stable log sum exp.\n",
    "        \"\"\"\n",
    "        if all(a == self.NEG_INF for a in args):\n",
    "                return self.NEG_INF\n",
    "        a_max = max(args)\n",
    "        lsp = math.log(sum(math.exp(a - a_max) for a in args))\n",
    "        return a_max + lsp\n",
    "    \n",
    "    def decode(self, probs, beam_size=100, blank=0):\n",
    "        \"\"\"\n",
    "        Performs inference for the given output probabilities.\n",
    "    \n",
    "        Arguments:\n",
    "                probs: The output probabilities (e.g. post-softmax) for each\n",
    "                    time step. Should be an array of shape (time x output dim).\n",
    "                beam_size (int): Size of the beam to use during inference.\n",
    "                blank (int): Index of the CTC blank label.\n",
    "    \n",
    "        Returns the output label sequence and the corresponding negative\n",
    "        log-likelihood estimated by the decoder.\n",
    "        \"\"\"\n",
    "        T, S = probs.shape\n",
    "        probs = np.log(probs)\n",
    "    \n",
    "        # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
    "        # Initialize the beam with the empty sequence, a probability of\n",
    "        # 1 for ending in blank and zero for ending in non-blank\n",
    "        # (in log space).\n",
    "        beam = [(tuple(), (0.0, self.NEG_INF))]\n",
    "    \n",
    "        for t in range(T): # Loop over time\n",
    "            if self.trace:\n",
    "                print('t:', t, file=sys.stderr)\n",
    "            # A default dictionary to store the next step candidates.\n",
    "            next_beam = self.make_new_beam()\n",
    "    \n",
    "            for s in range(S): # Loop over vocab\n",
    "                p = probs[t, s]\n",
    "    \n",
    "                # The variables p_b and p_nb are respectively the\n",
    "                # probabilities for the prefix given that it ends in a\n",
    "                # blank and does not end in a blank at this time step.\n",
    "                for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "    \n",
    "                    # If we propose a blank the prefix doesn't change.\n",
    "                    # Only the probability of ending in blank gets updated.\n",
    "                    if s == blank:\n",
    "                        n_p_b, n_p_nb = next_beam[prefix]\n",
    "                        n_p_b = self.logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "                        next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "                        continue\n",
    "    \n",
    "                    # Extend the prefix by the new character s and add it to\n",
    "                    # the beam. Only the probability of not ending in blank\n",
    "                    # gets updated.\n",
    "                    end_t = prefix[-1] if prefix else None\n",
    "                    n_prefix = prefix + (s,)\n",
    "                    n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "                    if s != end_t:\n",
    "                        n_p_nb = self.logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "                    else:\n",
    "                        # We don't include the previous probability of not ending\n",
    "                        # in blank (p_nb) if s is repeated at the end. The CTC\n",
    "                        # algorithm merges characters not separated by a blank.\n",
    "                        n_p_nb = self.logsumexp(n_p_nb, p_b + p)\n",
    "                        \n",
    "                    # *NB* this would be a good place to include an LM score.\n",
    "                    next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "    \n",
    "                    # If s is repeated at the end we also update the unchanged\n",
    "                    # prefix. This is the merging case.\n",
    "                    if s == end_t:\n",
    "                        n_p_b, n_p_nb = next_beam[prefix]\n",
    "                        n_p_nb = self.logsumexp(n_p_nb, p_nb + p)\n",
    "                        next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "    \n",
    "            # Sort and trim the beam before moving on to the\n",
    "            # next time-step.\n",
    "            beam = sorted(next_beam.items(),\n",
    "                            key=lambda x : self.logsumexp(*x[1]),\n",
    "                            reverse=True)\n",
    "            beam = beam[:beam_size]\n",
    "    \n",
    "        best = beam[0]\n",
    "        return best[0], -self.logsumexp(*best[1])\n",
    "    \n",
    "    def test(self):\n",
    "        np.random.seed(3)\n",
    "    \n",
    "        time = 6\n",
    "        output_dim = len(self.alphabet)\n",
    "    \n",
    "        probs = np.random.rand(time, output_dim)\n",
    "        probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "    \n",
    "        labels, score = self.decode(probs)\n",
    "        print(labels)\n",
    "        print(''.join([self.alphabet[i] for i in labels]))\n",
    "        print(\"Score {:.3f}\".format(score))\n",
    "        \n",
    "    def run(self, probs):\n",
    "        labels, score = self.decode(probs)\n",
    "        print(labels)\n",
    "        print(''.join([self.alphabet[i] for i in labels]))\n",
    "        print(\"Score {:.3f}\".format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 14, 7)\n",
      "eng\n",
      "Score 13.048\n"
     ]
    }
   ],
   "source": [
    "V = [c for c in ' abcdefghijklmnopqrstuvwxyz']\n",
    "dec = CTCDecoder(V)\n",
    "dec.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make an input matrix, emulating the output of the acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, sys\n",
    "\n",
    "C = ['c', 'c', 'a', 'a', 't', 't'] # sequence we want to output\n",
    "M = [] # matrix for output \n",
    "\n",
    "for c in C: \n",
    "        row = []\n",
    "        for v in V:\n",
    "                if v == c:\n",
    "                        row.append(10) # this is the best \n",
    "                else:\n",
    "                        row.append(random.randint(1,5)) # a random other value\n",
    "        nrow = [i/sum(row) for i in row] # normalise \n",
    "        M.append(nrow)\n",
    "\n",
    "M = np.array(M) # numpy-ise it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our TxV matrix (timesteps by vocabulary/alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04651163 0.02325581 0.01162791 0.11627907 0.04651163 0.03488372\n",
      "  0.02325581 0.05813953 0.02325581 0.03488372 0.01162791 0.05813953\n",
      "  0.04651163 0.02325581 0.02325581 0.01162791 0.02325581 0.04651163\n",
      "  0.04651163 0.03488372 0.03488372 0.03488372 0.03488372 0.04651163\n",
      "  0.02325581 0.04651163 0.03488372]\n",
      " [0.01123596 0.03370787 0.04494382 0.11235955 0.03370787 0.04494382\n",
      "  0.05617978 0.02247191 0.03370787 0.03370787 0.05617978 0.01123596\n",
      "  0.03370787 0.05617978 0.05617978 0.02247191 0.01123596 0.03370787\n",
      "  0.02247191 0.02247191 0.02247191 0.01123596 0.04494382 0.05617978\n",
      "  0.02247191 0.04494382 0.04494382]\n",
      " [0.01162791 0.11627907 0.04651163 0.03488372 0.04651163 0.04651163\n",
      "  0.01162791 0.02325581 0.04651163 0.03488372 0.04651163 0.02325581\n",
      "  0.03488372 0.05813953 0.05813953 0.01162791 0.01162791 0.04651163\n",
      "  0.04651163 0.01162791 0.01162791 0.03488372 0.01162791 0.03488372\n",
      "  0.04651163 0.04651163 0.04651163]\n",
      " [0.0625     0.125      0.025      0.0625     0.0125     0.0625\n",
      "  0.0125     0.025      0.05       0.0125     0.0125     0.05\n",
      "  0.025      0.0625     0.0625     0.0125     0.025      0.0125\n",
      "  0.025      0.0375     0.0375     0.0125     0.05       0.025\n",
      "  0.05       0.025      0.025     ]\n",
      " [0.05681818 0.05681818 0.03409091 0.01136364 0.04545455 0.03409091\n",
      "  0.03409091 0.02272727 0.05681818 0.05681818 0.01136364 0.01136364\n",
      "  0.02272727 0.01136364 0.03409091 0.01136364 0.04545455 0.05681818\n",
      "  0.05681818 0.03409091 0.11363636 0.03409091 0.01136364 0.02272727\n",
      "  0.04545455 0.02272727 0.04545455]\n",
      " [0.03448276 0.03448276 0.05747126 0.02298851 0.02298851 0.02298851\n",
      "  0.05747126 0.04597701 0.02298851 0.05747126 0.01149425 0.02298851\n",
      "  0.01149425 0.04597701 0.02298851 0.02298851 0.03448276 0.05747126\n",
      "  0.02298851 0.05747126 0.11494253 0.04597701 0.02298851 0.02298851\n",
      "  0.01149425 0.03448276 0.05747126]]\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 20)\n",
      "cat\n",
      "Score 10.810\n"
     ]
    }
   ],
   "source": [
    "dec.run(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 20)\n",
      "cat\n",
      "Score 10.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t: 0\n",
      "t: 1\n",
      "t: 2\n",
      "t: 3\n",
      "t: 4\n",
      "t: 5\n"
     ]
    }
   ],
   "source": [
    "dec.trace = True\n",
    "dec.run(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
