{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_aud = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/clips/'\n",
    "train_df = '/Users/tacha/iu_research/speech_recognition/asr_workshop/data/target-segments/ru/train.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform functions**\n",
    "\n",
    "Transform functions are the functions that are dealing with necessary input transformations e.g. feature extraction. They are feeded directly in the data loader. It helps to speed up data manipulation in contrast to reading all the file from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxlen(path, train_df):\n",
    "    fnames = pd.read_csv(train_df, sep='\\t')['path']\n",
    "    maxlen = 0\n",
    "    for n in tqdm(fnames):\n",
    "        waveform, sample_rate = torchaudio.load(os.path.join(path, n))\n",
    "        mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "        size = mfcc.shape[2]\n",
    "        if size > maxlen:\n",
    "            maxlen = size\n",
    "    print(\"Maxlen:\", maxlen)\n",
    "\n",
    "\n",
    "def extract_feats(path, maxlen=1083):\n",
    "    '''\n",
    "    Reads and processes one file at a time.\n",
    "    Args:\n",
    "        path: path to the file\n",
    "        maxlen: maximum length of the spectrogram for padding\n",
    "    '''\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "    #Calculate MFCC\n",
    "    mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "    #Calculate delta and double-delta\n",
    "    deltas = torchaudio.transforms.ComputeDeltas()(mfcc)\n",
    "    ddeltas = torchaudio.transforms.ComputeDeltas()(deltas)\n",
    "    res = torch.cat((mfcc, deltas, ddeltas), dim=1).squeeze(0)\n",
    "    #Normalize rows\n",
    "    s = torch.sum(res, dim=1, keepdim=True)\n",
    "    norm = torch.div(res, s)\n",
    "    mask = torch.ones(norm.shape[0], norm.shape[1])\n",
    "    padded_norm = nn.functional.pad(norm, pad=(0, maxlen-norm.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    padded_mask = nn.functional.pad(mask, pad=(0, maxlen-mask.shape[1], 0, 0), \n",
    "                                          mode=\"constant\",value=0)\n",
    "    return padded_norm, padded_mask\n",
    "\n",
    "def alphabet_enc(csv_path):\n",
    "    char2ind = {}\n",
    "    sents = pd.read_csv(csv_path, sep='\\t')['sentence']\n",
    "    chars = list(set([char for sent in sents for char in sent]))\n",
    "    for i in range(len(chars)):\n",
    "        char2ind[chars[i]] = i\n",
    "    char2ind[\"<eos>\"] = len(chars)+1 \n",
    "    return char2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, csv_path, aud_path, transform):\n",
    "        self.df = pd.read_csv(csv_path, sep='\\t')\n",
    "        self.aud_path = aud_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        fname = os.path.join(self.aud_path, self.df['path'][idx])\n",
    "        transcript = self.df['sentence'][idx].lower()\n",
    "\n",
    "        feat, mask = self.transform(fname)\n",
    "\n",
    "        sample = {'aud':feat, 'trans': transcript, 'mask':mask}\n",
    "        return sample\n",
    "    \n",
    "def weights(m):\n",
    "    '''\n",
    "    Intialize random weights\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed Architechture**\n",
    "\n",
    "Attention-based Sequence-to-Sequence model:\n",
    "\n",
    "![](/img/arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(120, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=256, \n",
    "                             num_layers=3, \n",
    "                             bidirectional=True)\n",
    "        self.h0 = torch.zeros(3*2, batch_size, 256)\n",
    "        self.c0 = torch.zeros(3*2, batch_size, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Pass through the first linear layer\n",
    "        outputs=[]\n",
    "        for i in range(x.shape[2]):\n",
    "            feature = x[:,:,i]\n",
    "            out = self.input_layer(feature)\n",
    "            out = torch.nn.LeakyReLU()(out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)\n",
    "        #Pass through LSTM layers\n",
    "        output, (hn, cn) = self.blstm(outputs, (self.h0, self.c0))\n",
    "        return output, (hn, cn)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size, char2ind):\n",
    "        super().__init__()\n",
    "        self.char2ind = char2ind\n",
    "        self.embed_layer = nn.Linear(512, 512)\n",
    "        self.blstm = nn.LSTM(input_size=512, \n",
    "                             hidden_size=512, \n",
    "                             num_layers=1)\n",
    "        self.h0 = torch.zeros(1, batch_size, 512)\n",
    "        self.c0 = torch.zeros(1, batch_size, 512)\n",
    "        self.y0 = torch.zeros(1, 512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2ind = alphabet_enc(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(32)\n",
    "encoder = encoder.to(device)\n",
    "encoder.apply(weights)\n",
    "\n",
    "cv_dataset = TrainData(train_df, train_aud, extract_feats)\n",
    "loader = data.DataLoader(cv_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(32, char2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0047,  0.0058, -0.0140,  ..., -0.0589,  0.0014,  0.0187],\n",
      "         [-0.0050,  0.0068, -0.0076,  ..., -0.0439, -0.0054, -0.0002],\n",
      "         [-0.0084,  0.0029, -0.0129,  ..., -0.0730, -0.0168,  0.0143],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0062,  0.0038, -0.0100,  ..., -0.0450, -0.0003,  0.0044],\n",
      "         [-0.0039,  0.0029, -0.0215,  ..., -0.0810,  0.0007,  0.0024]],\n",
      "\n",
      "        [[-0.0028,  0.0078, -0.0233,  ..., -0.0677,  0.0016,  0.0210],\n",
      "         [-0.0033,  0.0103, -0.0124,  ..., -0.0440, -0.0058,  0.0022],\n",
      "         [-0.0099,  0.0033, -0.0224,  ..., -0.0785, -0.0173,  0.0101],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0061,  0.0037, -0.0170,  ..., -0.0461,  0.0014,  0.0063],\n",
      "         [-0.0030,  0.0030, -0.0335,  ..., -0.0823,  0.0051, -0.0011]],\n",
      "\n",
      "        [[-0.0005,  0.0087, -0.0300,  ..., -0.0797,  0.0032,  0.0190],\n",
      "         [-0.0003,  0.0120, -0.0156,  ..., -0.0441, -0.0061,  0.0038],\n",
      "         [-0.0087,  0.0047, -0.0301,  ..., -0.0817, -0.0157, -0.0020],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [-0.0054,  0.0014, -0.0224,  ..., -0.0475,  0.0042,  0.0064],\n",
      "         [-0.0017,  0.0026, -0.0373,  ..., -0.0807,  0.0075, -0.0126]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0060,  0.0130, -0.0202,  ..., -0.0430, -0.0101, -0.0029],\n",
      "         [ 0.0060,  0.0130, -0.0202,  ..., -0.0430, -0.0101, -0.0029],\n",
      "         [ 0.0060,  0.0130, -0.0202,  ..., -0.0430, -0.0101, -0.0029],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0060,  0.0130, -0.0202,  ..., -0.0430, -0.0101, -0.0029],\n",
      "         [ 0.0060,  0.0130, -0.0202,  ..., -0.0430, -0.0101, -0.0029]],\n",
      "\n",
      "        [[ 0.0054,  0.0130, -0.0203,  ..., -0.0373, -0.0095, -0.0047],\n",
      "         [ 0.0054,  0.0130, -0.0203,  ..., -0.0373, -0.0095, -0.0047],\n",
      "         [ 0.0054,  0.0130, -0.0203,  ..., -0.0373, -0.0095, -0.0047],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0054,  0.0130, -0.0203,  ..., -0.0373, -0.0095, -0.0047],\n",
      "         [ 0.0054,  0.0130, -0.0203,  ..., -0.0373, -0.0095, -0.0047]],\n",
      "\n",
      "        [[ 0.0053,  0.0133, -0.0208,  ..., -0.0246, -0.0068, -0.0047],\n",
      "         [ 0.0053,  0.0133, -0.0208,  ..., -0.0246, -0.0068, -0.0047],\n",
      "         [ 0.0053,  0.0133, -0.0208,  ..., -0.0246, -0.0068, -0.0047],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [ 0.0053,  0.0133, -0.0208,  ..., -0.0246, -0.0068, -0.0047],\n",
      "         [ 0.0053,  0.0133, -0.0208,  ..., -0.0246, -0.0068, -0.0047]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "tensor([[[-1.3273e-02,  8.2946e-03, -1.4355e-02,  ..., -6.8423e-02,\n",
      "           2.9131e-02, -1.8638e-02],\n",
      "         [-4.9714e-03,  6.8540e-03, -7.5966e-03,  ..., -4.3872e-02,\n",
      "          -5.3601e-03, -1.9364e-04],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-4.9647e-03,  6.8570e-03, -7.5258e-03,  ..., -4.4244e-02,\n",
      "          -6.1772e-03,  1.2777e-04],\n",
      "         [-5.1685e-03,  7.9344e-04, -1.2416e-02,  ..., -6.4288e-02,\n",
      "          -4.5242e-03,  1.1869e-02]],\n",
      "\n",
      "        [[-1.9098e-02,  6.8667e-03, -1.9244e-02,  ..., -5.8055e-02,\n",
      "           4.0469e-02, -3.0588e-02],\n",
      "         [-3.2868e-03,  1.0320e-02, -1.2357e-02,  ..., -4.3905e-02,\n",
      "          -5.7224e-03,  2.1326e-03],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-3.2964e-03,  1.0326e-02, -1.2229e-02,  ..., -4.4454e-02,\n",
      "          -6.8891e-03,  2.7418e-03],\n",
      "         [-1.8061e-03, -1.9670e-03, -2.1116e-02,  ..., -7.1214e-02,\n",
      "          -4.0496e-03,  1.2763e-02]],\n",
      "\n",
      "        [[-1.9449e-02,  9.4209e-03, -2.4538e-02,  ..., -4.2300e-02,\n",
      "           4.6726e-02, -4.7059e-02],\n",
      "         [-2.9657e-04,  1.2032e-02, -1.5572e-02,  ..., -4.3985e-02,\n",
      "          -6.0368e-03,  3.6808e-03],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [-3.5425e-04,  1.2053e-02, -1.5377e-02,  ..., -4.4804e-02,\n",
      "          -7.6729e-03,  4.7522e-03],\n",
      "         [ 4.6329e-03, -7.3383e-03, -2.7844e-02,  ..., -7.8404e-02,\n",
      "          -3.1004e-03,  1.0896e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.0673e-02,  1.2550e-01, -3.1034e-02,  ...,  6.8716e-02,\n",
      "           4.1199e-02, -2.0475e-02],\n",
      "         [ 5.9676e-03,  1.3031e-02, -2.0243e-02,  ..., -4.3013e-02,\n",
      "          -1.0082e-02, -2.8625e-03],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 5.9676e-03,  1.3031e-02, -2.0243e-02,  ..., -4.3013e-02,\n",
      "          -1.0082e-02, -2.8625e-03],\n",
      "         [ 5.9676e-03,  1.3031e-02, -2.0243e-02,  ..., -4.3013e-02,\n",
      "          -1.0082e-02, -2.8625e-03]],\n",
      "\n",
      "        [[-2.8981e-02,  1.3205e-01, -1.9507e-02,  ...,  5.9772e-02,\n",
      "           2.4285e-02, -1.9489e-02],\n",
      "         [ 5.4273e-03,  1.2986e-02, -2.0320e-02,  ..., -3.7310e-02,\n",
      "          -9.5315e-03, -4.7140e-03],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 5.4273e-03,  1.2986e-02, -2.0320e-02,  ..., -3.7310e-02,\n",
      "          -9.5315e-03, -4.7140e-03],\n",
      "         [ 5.4273e-03,  1.2986e-02, -2.0320e-02,  ..., -3.7310e-02,\n",
      "          -9.5315e-03, -4.7140e-03]],\n",
      "\n",
      "        [[-1.4006e-02,  1.3739e-01, -8.7524e-03,  ...,  4.1218e-02,\n",
      "           8.4309e-03, -1.4627e-02],\n",
      "         [ 5.3309e-03,  1.3285e-02, -2.0758e-02,  ..., -2.4621e-02,\n",
      "          -6.8480e-03, -4.7363e-03],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [ 5.3309e-03,  1.3285e-02, -2.0758e-02,  ..., -2.4621e-02,\n",
      "          -6.8480e-03, -4.7363e-03],\n",
      "         [ 5.3309e-03,  1.3285e-02, -2.0758e-02,  ..., -2.4621e-02,\n",
      "          -6.8480e-03, -4.7363e-03]]], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-de6cacb882c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aud'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prog/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-0dbd58617aa1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#Pass through LSTM layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prog/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/prog/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    x = batch['aud'].to(device)\n",
    "    out, (h, c) = encoder(x)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
